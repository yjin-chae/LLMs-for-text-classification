{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, pipeline\n",
    "from transformers.pipelines.pt_utils import KeyDataset\n",
    "import torch, datasets, pandas as pd\n",
    "\n",
    "task = \"fb\" # \"semeval\"\n",
    "device = \"cuda\"\n",
    "\n",
    "########################## Model list\n",
    "# \"google/flan-t5-xxl\"\n",
    "# \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "# \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "# \"meta-llama/Meta-Llama-3-70B-Instruct\"\n",
    "\n",
    "target = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(target, truncate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zero-shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if task == \"fb\":\n",
    "    prompt3=\"This statement may express a STANCE towards two politicians, Trump and Clinton. Stance represents the attitude expressed towards them. The stance options are Favor, Against or None. Provide the answer in the following format: {Trump: STANCE, Clinton: STANCE}\\n\\n\"\n",
    "else:\n",
    "    prompt3=\"This statement contains a TARGET and a STANCE. The target is a politician and the stance represents the attitude expressed about them. The target options are Trump or Clinton and stance options are Favor, Against or None. Provide the answer in the following format: {TARGET, STANCE}\\n\\n\"\n",
    "    \n",
    "test = pd.read_csv(f\"data/{task}_test.csv\")\n",
    "test['prompt'] = prompt3 + test['prompt']\n",
    "test['chat_text'] = test['prompt'].apply(lambda x:tokenizer.apply_chat_template([{\"role\":\"user\", \"content\":x}], tokenize=False, add_generation_prompt=True))\n",
    "\n",
    "test_ds = datasets.Dataset.from_pandas(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\"text2text-generation\" if target == \"google/flan-t5-xxl\" else \"text-generation\",\n",
    "                model=target,\n",
    "                tokenizer=tokenizer,\n",
    "                device_map=device,\n",
    "                torch_dtype=torch.bfloat16)\n",
    "\n",
    "pipe.tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_labels = []\n",
    "for out in pipe(KeyDataset(test_ds, \"chat_text\"),\n",
    "                add_special_tokens=True,\n",
    "                return_full_text=False,\n",
    "                do_sample=True,\n",
    "                temperature=0.1,\n",
    "                max_new_tokens=20,\n",
    "                batch_size=1):\n",
    "    output_labels.append(out[0]) \n",
    "    \n",
    "test_preds = pd.concat([test, pd.DataFrame.from_dict(output_labels)], axis=1)\n",
    "test_preds.to_csv(f\"predicted_labels/{task}_{target.split(\"/\")[-1]}_zero.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
