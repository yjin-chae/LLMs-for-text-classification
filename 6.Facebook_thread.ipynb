{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments, pipeline \n",
    "import torch, datasets, pandas as pd\n",
    "from trl import SFTTrainer\n",
    "from transformers.pipelines.pt_utils import KeyDataset\n",
    "from peft import LoraConfig\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"You will receive JSON inputs representing discussion threads from social media during the 2016 US Presidential election. Each thread includes a post, one comment about the post, and up to five replies to the comment. Your task is to identify the stance expressed towards two politicians, Donald Trump and Hillary Clinton, in the comment and each reply. Each text may express a stance towards one, both, or none of the politicians. You will always provide a stance towards each politician separately.\n",
    "\n",
    "Stance Options:\n",
    "\n",
    "    Support: Positive attitude towards the politician.\n",
    "    Oppose: Negative attitude towards the politician.\n",
    "    Neither: No clear stance or irrelevant content.\n",
    "\n",
    "Instructions:\n",
    "\n",
    "    - Identify the stance for Trump and Clinton in the comment and each reply using the stance options provided.\n",
    "    - Always provide a stance even if the content is offensive or ambiguous.\n",
    "    - There will be between zero and five replies to each comment. If there are fewer than five replies, provide stances for the available replies only.\n",
    "    \n",
    "Output Format: Strictly follow this JSON format. Replace the STANCE placeholder with the actual stance. Do not add any other tokens:\n",
    "\n",
    "\n",
    "{\n",
    "  \"comment\": {\n",
    "    \"stanceTrump\": \"STANCE\",\n",
    "    \"stanceClinton\": \"STANCE\"\n",
    "  },\n",
    "  \"replies\": [\n",
    "    {\n",
    "      \"reply_id\": 1,\n",
    "      \"stanceTrump\": \"STANCE\",\n",
    "      \"stanceClinton\": \"STANCE\"\n",
    "    },\n",
    "    {\n",
    "      \"reply_id\": 2,\n",
    "      \"stanceTrump\": \"STANCE\",\n",
    "      \"stanceClinton\": \"STANCE\"\n",
    "    },\n",
    "    ...\n",
    "  ]\n",
    "}\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load thread data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_thread = pd.read_csv(\"data/thread_test.csv\") # For GPT-4o JSON zero-shot\n",
    "test_cleaned = pd.read_csv(\"data/thread_test_cleaned.csv\") # For GPT-4o Baseline zero-shot\n",
    "train_texts = pd.read_csv(\"data/thread_train_chat_texts.csv\") # For llama3\n",
    "test_texts = pd.read_csv(\"data/thread_test_chat_texts.csv\") # For llama3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Llama 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################## Model list\n",
    "# \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "# \"meta-llama/Meta-Llama-3-70B-Instruct\"\n",
    "    \n",
    "target = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "device = \"cuda\"\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "torch_dtype = torch.bfloat16\n",
    "quant_storage_dtype = torch.bfloat16\n",
    "\n",
    "quantization_config = BitsAndBytesConfig( \n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=torch_dtype,\n",
    "            bnb_4bit_quant_storage=quant_storage_dtype, \n",
    "        )\n",
    "    \n",
    "tokenizer = AutoTokenizer.from_pretrained(target)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = datasets.Dataset.from_pandas(train_texts)\n",
    "test_ds = datasets.Dataset.from_pandas(test_texts).map(lambda x:{\"chat_text\":tokenizer.apply_chat_template(x['messages'], tokenize=False, add_generation_prompt=True)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    target,\n",
    "    device_map=device,\n",
    "    torch_dtype=quant_storage_dtype,\n",
    "    quantization_config=quantization_config,\n",
    "    use_cache=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero-shot JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\"text-generation\",\n",
    "                model=model,\n",
    "                tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_labels = []\n",
    "for out in pipe(KeyDataset(test_ds, \"chat_text\"),\n",
    "                add_special_tokens=True,\n",
    "                return_full_text=False,\n",
    "                do_sample=True,\n",
    "                temperature=0.1,\n",
    "                max_new_tokens=8192,\n",
    "                batch_size=1):\n",
    "    output_labels.append(out[0])\n",
    "\n",
    "preds = pd.DataFrame.from_dict(output_labels)\n",
    "test_preds = pd.concat([test_texts, preds], axis=1)\n",
    "test_preds.to_csv(f\"predicted_labels/thread_{target.split(\"/\")[-1]}_zero.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instruction fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"open_weights/thread/{target}\",\n",
    "    logging_dir=f\"open_weights/thread/{target}\",\n",
    "    num_train_epochs=4,\n",
    "    learning_rate=2e-4,\n",
    "    lr_scheduler_type=\"constant\",\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    gradient_checkpointing=True,\n",
    "    gradient_checkpointing_kwargs={\"use_reentrant\":True},\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=0.5,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=0.5\n",
    "    )\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=64,\n",
    "    lora_dropout=0.1 if target == \"meta-llama/Meta-Llama-3-70B-Instruct\" else 0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=\"all-linear\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    peft_config=peft_config,\n",
    "    train_dataset=train_ds,\n",
    "    max_seq_length=8192,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_sft = pipeline(\"text-generation\",\n",
    "                model=model,\n",
    "                tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_labels = []\n",
    "for out in pipe_sft(KeyDataset(test_ds, \"chat_text\"),\n",
    "                add_special_tokens=True,\n",
    "                return_full_text=False,\n",
    "                do_sample=True,\n",
    "                temperature=0.1,\n",
    "                max_new_tokens=8192,\n",
    "                batch_size=1):\n",
    "    output_labels.append(out[0])\n",
    "\n",
    "preds = pd.DataFrame.from_dict(output_labels)\n",
    "test_preds = pd.concat([test_texts, preds], axis=1)\n",
    "test_preds.to_csv(f\"predicted_labels/thread_{target.split(\"/\")[-1]}_instruction_tuned.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT-4o"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero-shot JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_output = []\n",
    "        \n",
    "for j, thread in enumerate(test_thread.iterrows()):\n",
    "    message = thread[1]['thread']\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        temperature=0,\n",
    "        messages=[\n",
    "            {\"role\":\"system\", \"content\":system_prompt},\n",
    "            {\"role\": \"user\", \"content\": message}\n",
    "            ]\n",
    "        )\n",
    "    print(completion.choices[0].message)\n",
    "    generated_output.append(completion.choices[0].message.content)\n",
    "preds = pd.DataFrame({\"generated_text\":generated_output})\n",
    "test_thread_preds = pd.concat([test_thread, preds], axis=1)\n",
    "test_thread_preds.to_csv(f\"predicted_labels/thread_gpt4o_zero.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero-shot Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"../creds.json\") as js:\n",
    "    # api_key = json.load(js)['OPENAI_API_KEY']\n",
    "    \n",
    "client = OpenAI(api_key=api_key)\n",
    "prompt3 = \"This statement may express a STANCE towards two politicians, Trump and Clinton. Stance represents the attitude expressed towards them. The stance options are Support, Oppose or Neither. Provide the answer in the following format: {Trump: STANCE, Clinton: STANCE}\\n\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_output = []\n",
    "        \n",
    "for thread in test_cleaned.iterrows():\n",
    "    message = prompt3 + thread[1]['text']\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        temperature=0,\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": message}\n",
    "            ]\n",
    "        )\n",
    "    generated_output.append(completion.choices[0].message.content)\n",
    "\n",
    "test_cleaned_preds = pd.concat([test_cleaned, pd.DataFrame({\"preds\":generated_output})], axis=1)\n",
    "test_cleaned_preds.to_csv(f\"predicted_labels/thread_gpt4o_baseline.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
