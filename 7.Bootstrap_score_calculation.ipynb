{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, recall_score, precision_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import statistics\n",
    "import joblib\n",
    "\n",
    "def calc_scores(preds, average):\n",
    "    preds_sample = preds.sample(frac=1, replace=True)\n",
    "    f1 = f1_score(preds_sample[\"completion\"], preds_sample[\"preds\"], average=average)\n",
    "    recall = recall_score(preds_sample[\"completion\"], preds_sample[\"preds\"], average=average)\n",
    "    precision = precision_score(preds_sample[\"completion\"], preds_sample[\"preds\"], average=average)\n",
    "    return f1, recall, precision\n",
    "\n",
    "\n",
    "def bootstrapped_ci(preds, model, regime, average=\"weighted\", n_replicate=10000, n_cores=10):\n",
    "    f1_estimate = f1_score(preds[\"completion\"], preds[\"preds\"], average=average)\n",
    "    recall_estimate = recall_score(preds[\"completion\"], preds[\"preds\"], average=average)\n",
    "    precision_estimate = precision_score(preds[\"completion\"], preds[\"preds\"], average=average)\n",
    "    \n",
    "    out = np.vstack(joblib.Parallel(n_jobs=n_cores)(joblib.delayed(calc_scores)(preds, average) for _ in range(n_replicate)))\n",
    "    f1, recall, precision = out[:,0], out[:,1], out[:,2]\n",
    "    \n",
    "    ci = pd.DataFrame({\"model\":[model],\n",
    "                    \"regime\":[regime],\n",
    "                    \"f1_estimate\":[f1_estimate],\n",
    "                    \"f1_ci.low\":[np.percentile(f1, 2.5)],\n",
    "                    \"f1_ci.high\":[np.percentile(f1, 97.5)],\n",
    "                    \"recall_estimate\":[recall_estimate],\n",
    "                    \"recall_ci.low\":[np.percentile(recall, 2.5)],\n",
    "                    \"recall_ci.high\":[np.percentile(recall, 97.5)],\n",
    "                    \"precision_estimate\":[precision_estimate],\n",
    "                    \"precision_ci.low\":[np.percentile(precision, 2.5)],\n",
    "                    \"precision_ci.high\":[np.percentile(precision, 97.5)]\n",
    "                    }\n",
    "                    )\n",
    "    return ci\n",
    "\n",
    "\n",
    "def thread_to_df(obj):\n",
    "    import json\n",
    "    from json_repair import repair_json\n",
    "\n",
    "    obj['json'] = obj['preds'].replace({\", {\":\", \",\n",
    "                                                 \"}}\":\"}\",\n",
    "                                                 \"```json\":\"\",\n",
    "                                                 \"```\":\"\"}, regex=True).str.strip()\n",
    "    preds = pd.DataFrame()\n",
    "    for i, thread in enumerate(obj.iterrows()):\n",
    "        thread = repair_json(thread[1]['json'])\n",
    "\n",
    "        try:\n",
    "            comment = pd.DataFrame.from_dict([json.loads(thread)['comment']])\n",
    "            reply = pd.DataFrame.from_dict(json.loads(thread)['replies'])\n",
    "            comment['reply_id'] = 0\n",
    "            output = pd.concat([comment, reply],axis=0)\n",
    "            output['post_id'] = i+1\n",
    "        except KeyError:\n",
    "            comment = pd.DataFrame.from_dict([json.loads(thread)['comment']])\n",
    "            comment['reply_id'] = 0\n",
    "            output = comment\n",
    "            output['post_id'] = i+1\n",
    "        preds = pd.concat([preds, output],axis=0)\n",
    "    preds[\"preds\"] = preds[\"stanceTrump\"] + \", \" + preds[\"stanceClinton\"]\n",
    "    \n",
    "    return preds.reset_index(drop=True)\n",
    "\n",
    "\n",
    "def thread_ci(test_cleaned, preds_cleaned, model, regime):\n",
    "    cis = pd.DataFrame()\n",
    "    test_cleaned[\"reply_id_sum\"] = test_cleaned.groupby(\"post_id\")[\"reply_id\"].transform(\"sum\")\n",
    "    preds_cleaned[\"reply_id_sum\"] = preds_cleaned.groupby(\"post_id\")[\"reply_id\"].transform(\"sum\")\n",
    "                \n",
    "    preds = pd.DataFrame({\"completion\": test_cleaned[\"completion\"],\n",
    "                            \"preds\": preds_cleaned[\"preds\"]})\n",
    "    \n",
    "    ci = bootstrapped_ci(preds=preds, model=model, regime=regime)\n",
    "    ci[\"reply_set\"] = \"overall\"\n",
    "    ci[\"reply_set_id\"] = \"overall\"\n",
    "    ci[\"dataset\"] = \"thread\"\n",
    "    cis = pd.concat([cis, ci], axis=0)\n",
    "\n",
    "    for reply in [0, 1, 2, 3, 4, 5]:\n",
    "        reply_id_sum = reply * (reply+1) / 2\n",
    "\n",
    "        test_cleaned_subset = test_cleaned.loc[test_cleaned['reply_id_sum']==reply_id_sum,:]\n",
    "        preds_cleaned_subset = preds_cleaned.loc[preds_cleaned['reply_id_sum']==reply_id_sum,:]\n",
    "        \n",
    "        preds = pd.DataFrame({\"completion\": test_cleaned_subset[\"completion\"],\n",
    "                            \"preds\": preds_cleaned_subset[\"preds\"]})\n",
    "        \n",
    "        ci = bootstrapped_ci(preds=preds, model=model, regime=regime)\n",
    "        ci[\"reply_set\"] = reply\n",
    "        ci[\"reply_set_id\"] = \"overall\"\n",
    "        ci[\"dataset\"] = \"thread\"\n",
    "        cis = pd.concat([cis, ci], axis=0)\n",
    "        \n",
    "        for r in range(reply+1):\n",
    "            test_cleaned_subset_r = test_cleaned_subset.loc[test_cleaned_subset['reply_id']==r,:]\n",
    "            preds_cleaned_subset_r = preds_cleaned_subset.loc[preds_cleaned_subset['reply_id']==r,:]\n",
    "            \n",
    "            preds = pd.DataFrame({\"completion\": test_cleaned_subset_r[\"completion\"],\n",
    "                            \"preds\": preds_cleaned_subset_r[\"preds\"]})\n",
    "        \n",
    "            ci = bootstrapped_ci(preds=preds, model=model, regime=regime)\n",
    "            ci[\"reply_set\"] = reply\n",
    "            ci[\"reply_set_id\"] = r\n",
    "            ci[\"dataset\"] = \"thread\"\n",
    "            cis = pd.concat([cis, ci], axis=0)\n",
    "        \n",
    "    for r in [0, 1, 2, 3, 4, 5]:\n",
    "        test_cleaned_r = test_cleaned.loc[test_cleaned['reply_id']==r,:]\n",
    "        preds_cleaned_r = preds_cleaned.loc[preds_cleaned['reply_id']==r,:]\n",
    "        \n",
    "        preds = pd.DataFrame({\"completion\": test_cleaned_r[\"completion\"],\n",
    "                        \"preds\": preds_cleaned_r[\"preds\"]})\n",
    "    \n",
    "        ci = bootstrapped_ci(preds=preds, model=model, regime=regime)\n",
    "        ci[\"reply_set\"] = \"overall\"\n",
    "        ci[\"reply_set_id\"] = r\n",
    "        ci[\"dataset\"] = \"thread\"\n",
    "        cis = pd.concat([cis, ci], axis=0)\n",
    "                \n",
    "    return cis\n",
    "\n",
    "\n",
    "def combine_prediction(df):\n",
    "    df['reply_set'] = df.groupby('post_id')['reply_id'].transform('max')\n",
    "    df = df.pivot(index=['post_id', 'reply_set'], columns='reply_id', values='preds').reset_index().fillna(\"\")\n",
    "    \n",
    "    df['preds'] = df[0] + \", \" + df[1] + \", \" + df[2] + \", \" + df[3] + \", \" + df[4]\n",
    "    \n",
    "    return df\n",
    "\n",
    "target = [\n",
    "    \"bert-base-uncased\",\n",
    "    \"sentence-transformers/all-mpnet-base-v2\"\n",
    "    \"microsoft/deberta-v3-base\",\n",
    "    \"DeBERTa-v3-base-MNLI\",\n",
    "    \"google/flan-t5-xxl\",\n",
    "    \"mistralai/Mistral-7B-v0.3\",\n",
    "    \"mistralai/Mistral-7B-Instruct-v0.3\",\n",
    "    \"meta-llama/Meta-Llama-3-8B\",\n",
    "    \"meta-llama/Meta-Llama-3-70B\",\n",
    "    \"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "    \"meta-llama/Meta-Llama-3-70B-Instruct\",\n",
    "    \"davinci\",\n",
    "    \"ada\",\n",
    "    \"gpt4o\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cis = pd.DataFrame()\n",
    "x_label = {\"prompt1\":\"Minimal\",\n",
    "           \"prompt2\":\"Sentence\",\n",
    "           \"prompt3\":\"Context\"\n",
    "           }\n",
    "\n",
    "for task in [\"fb\", \"semeval\"]:\n",
    "    for num_data in [\"prompt1\", \"prompt2\", \"prompt3\"]:\n",
    "        for t in [\"Trump\", \"Clinton\", \"Joint\"]:\n",
    "            preds = pd.read_csv(f\"predicted_labels/{task}_davinci_zero_{num_data}.csv\")\n",
    "            \n",
    "            if task == \"fb\":\n",
    "                if t != \"Joint\":\n",
    "                    idx = 0 if t == \"Trump\" else 1\n",
    "                    preds[\"completion\"] = preds[\"completion\"].apply(lambda x:x.split(\", \")[idx])\n",
    "                    preds[\"preds\"] = preds[\"preds\"].apply(lambda x:x.split(\", \")[idx])\n",
    "                    \n",
    "            elif task == \"semeval\":\n",
    "                if t != \"Joint\":\n",
    "                    preds = preds.loc[preds[\"completion\"].str.startswith(f\"{t.title()}\"),:]\n",
    "                    \n",
    "            ci = bootstrapped_ci(preds, \"davinci\", num_data)\n",
    "            ci[\"target\"] = t\n",
    "            ci[\"dataset\"] = task\n",
    "            cis = pd.concat([cis, ci],axis=0)\n",
    "            \n",
    "cis['regime'] = cis['regime'].replace(x_label, regex=True)\n",
    "cis.to_csv(\"tables-and-plots/raw_scores/prompt_engineering.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bootstrapping for Tasks 1 and 2: Twitter and Facebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = \"fb\" # \"semeval\"\n",
    "PATH = \"predicted_labels/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cis = pd.DataFrame()\n",
    "\n",
    "## Zero shot\n",
    "for m in target:\n",
    "    for f in glob(f\"{task}_{m.split(\"/\")[-1]}_zero.csv\", root_dir=PATH):\n",
    "        preds = pd.read_csv(PATH+f)\n",
    "        \n",
    "        ci = bootstrapped_ci(preds=preds, model=m.split(\"/\")[-1], regime=\"zero-shot\")\n",
    "        ci[\"dataset\"] = task\n",
    "        cis = pd.concat([cis, ci], axis=0)\n",
    "            \n",
    "## One shot\n",
    "num_data = \"one-shot\"\n",
    "for m in target:\n",
    "    pooled = pd.DataFrame()\n",
    "    \n",
    "    for i, f in enumerate(glob(f\"{task}_{m.split(\"/\")[-1]}_one_eg_*.csv\", root_dir=PATH)):\n",
    "        preds = pd.read_csv(PATH+f)\n",
    "        pooled = pd.concat([pooled, pd.DataFrame({f\"preds_{i}\":preds['preds']})],axis=1)\n",
    "    preds_mode = pooled.mode(axis=1)[0]\n",
    "    preds[\"preds\"] = preds_mode\n",
    "    \n",
    "    ci = bootstrapped_ci(preds, model=m.split(\"/\")[-1], regime=\"one-shot\")\n",
    "    ci[\"dataset\"] = task\n",
    "    cis = pd.concat([cis, ci], axis=0)\n",
    "        \n",
    "## Fine tuned\n",
    "regime = [\"10\", \"100\", \"1000\", \"all\"]\n",
    "for m in target:\n",
    "    for num_data in regime:\n",
    "        for f in glob(f\"{task}_{m.split(\"/\")[-1]}_{num_data}.csv\", root_dir=PATH):\n",
    "            preds = pd.read_csv(PATH+f)\n",
    "            \n",
    "            ci = bootstrapped_ci(preds, model=m.split(\"/\")[-1], regime=num_data)\n",
    "            ci[\"dataset\"] = task\n",
    "            cis = pd.concat([cis, ci], axis=0)\n",
    "            \n",
    "cis.to_csv(\"tables-and-plots/raw_scores/tasks_1_2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bootstrapping for Task 3: Facebook comment threads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cleaned = pd.read_csv(\"data/thread_test_cleaned.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JSON zero-shot and instruction-tuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cis = pd.DataFrame()\n",
    "for num_data in [\"zero\", \"instruction_tuned\"]:\n",
    "    for m in [\"Meta-Llama-3-8B-Instruct\", \"Meta-Llama-3-70B-Instruct\", \"gpt4o\"]:    \n",
    "        if not (num_data == \"instruction_tuned\" and m == \"gpt4o\"):\n",
    "            preds = pd.read_csv(f\"predicted_labels/thread_{m}_{num_data}.csv\")\n",
    "            preds_cleaned = thread_to_df(preds)\n",
    "            preds_cleaned.to_csv(f\"predicted_labels/thread_{m}_{num_data}_cleaned.csv\", index=False)\n",
    "            combine_prediction(preds_cleaned).to_csv(f\"predicted_labels/thread_{m}_{num_data}_thread_joint_score.csv\", index=False)\n",
    "            ci = thread_ci(test_cleaned, preds_cleaned, m, num_data)\n",
    "            cis = pd.concat([cis, ci],axis=0)\n",
    "\n",
    "cis.to_csv(\"tables-and-plots/raw_scores/task3.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT-4o baseline zero-shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_baseline = pd.read_csv(\"predicted_labels/thread_gpt4o_baseline.csv\")\n",
    "combine_prediction(preds_baseline).to_csv(f\"predicted_labels/thread_gpt4o_baseline_thread_joint_score.csv\", index=False)\n",
    "cis = thread_ci(test_cleaned, preds_baseline, \"gpt4o\", \"baseline\")\n",
    "cis.to_csv(\"tables-and-plots/raw_scores/task3_baseline.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Threadwise joint score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_baseline_thread_joint = combine_prediction(test_cleaned.rename(columns={\"completion\":\"preds\"}))\n",
    "cis = pd.DataFrame()\n",
    "for num_data in [\"zero\", \"instruction_tuned\", \"baseline\"]:\n",
    "    for m in [\"Meta-Llama-3-8B-Instruct\", \"Meta-Llama-3-70B-Instruct\", \"gpt4o\"]:    \n",
    "        try:\n",
    "            preds_cleaned = pd.read_csv(f\"predicted_labels/thread_{m}_{num_data}_thread_joint_score.csv\")\n",
    "        except FileNotFoundError:\n",
    "            continue\n",
    "    \n",
    "        preds = pd.DataFrame({\"completion\": test_baseline_thread_joint[\"preds\"],\n",
    "                              \"preds\": preds_cleaned[\"preds\"]})\n",
    "                            \n",
    "        ci = bootstrapped_ci(preds)\n",
    "        ci[\"reply_set\"] = \"overall\"\n",
    "        ci[\"dataset\"] = \"thread\"\n",
    "        cis = pd.concat([cis, ci], axis=0)\n",
    "        \n",
    "        for reply in [0, 1, 2, 3, 4, 5]:\n",
    "            test_baseline_thread_joint_subset = test_baseline_thread_joint.loc[test_baseline_thread_joint['reply_set']==reply,:]\n",
    "            preds_cleaned_subset = preds_cleaned.loc[preds_cleaned['reply_set']==reply,:]\n",
    "            \n",
    "            preds = pd.DataFrame({\"completion\": test_baseline_thread_joint_subset[\"preds\"],\n",
    "                                \"preds\": preds_cleaned_subset[\"preds\"]})\n",
    "            \n",
    "            ci = bootstrapped_ci(preds)\n",
    "            ci[\"reply_set\"] = reply\n",
    "            ci[\"dataset\"] = \"thread\"\n",
    "            cis = pd.concat([cis, ci], axis=0)\n",
    "\n",
    "cis.to_csv(\"tables-and-plots/raw_scores/task3_threadwise_joint.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
